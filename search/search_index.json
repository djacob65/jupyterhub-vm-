{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Generation of a virtual machine (VM) from an ubuntu server installation image (ISO file) containing The Littlest JupyerHub (TLJH) with the help of Packer , Vagrant and Ansible tools for using with VirtualBox or OpenStack . All the tools used are exclusively free and open-source. The Littlest JupyterHub , a recent and evolving distribution designed for smaller deployments, is a lightweight method to install JupyterHub on a single virtual machine. The Littlest JupyterHub (also known as TLJH), provides a guide with information on creating a VM on several cloud providers, as well as installing and customizing JupyterHub so that users may access it at a public URL. Overview \u00b6 This work was carried on a desktop computer running Windows10 / Cygwin. But any machine (laptop, desktop, server, ...) under MacOS or UNIX-like can be perfectly suitable. Please note that this requires technical skills in installing applications and assumes that you are comfortable editing configuration files. Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . To build our final VM, i.e. the VM that will be instantiated for use locally ( type 2 virtualization ) or on a remote server (Datacenter or Cloud), we will proceed in two main steps for the creation. The first step will consist in the building of a base VM (Base box). The second step will consist, from the base VM, in installing and configuring all the system and application tools (packages) necessary to make JupyterHub work correctly from a web browser. A third step will describe how to instantiate the final VM on the Genouest cloud. The figure below gives an overview: The three main stages Building the base VM (Base box) for Vagrant Building the final VM Using the final VM on an Openstack cloud Pipeline In green, the pipeline path for the creation, storage and instantiation of the virtual machine. As input to the pipeline, an ISO file corresponding to the chosen operating system and downloaded from the Internet. At the output, an instance of the virtual machine operational on Genouest's Openstack cloud. The different layers The configuration layer corresponds to all the files and scripts deposited in the github. The creation layer corresponds to all the tools installed and used on the user's local machine (except for Ansible which is installed on the virtual machine but which could very well be installed on the local machine). The storage layer corresponds to the storage sites of the VMs (base and final). The instantiation layer corresponds to the instantiated virtual machine.","title":"Purpose"},{"location":"#overview","text":"This work was carried on a desktop computer running Windows10 / Cygwin. But any machine (laptop, desktop, server, ...) under MacOS or UNIX-like can be perfectly suitable. Please note that this requires technical skills in installing applications and assumes that you are comfortable editing configuration files. Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . To build our final VM, i.e. the VM that will be instantiated for use locally ( type 2 virtualization ) or on a remote server (Datacenter or Cloud), we will proceed in two main steps for the creation. The first step will consist in the building of a base VM (Base box). The second step will consist, from the base VM, in installing and configuring all the system and application tools (packages) necessary to make JupyterHub work correctly from a web browser. A third step will describe how to instantiate the final VM on the Genouest cloud. The figure below gives an overview: The three main stages Building the base VM (Base box) for Vagrant Building the final VM Using the final VM on an Openstack cloud Pipeline In green, the pipeline path for the creation, storage and instantiation of the virtual machine. As input to the pipeline, an ISO file corresponding to the chosen operating system and downloaded from the Internet. At the output, an instance of the virtual machine operational on Genouest's Openstack cloud. The different layers The configuration layer corresponds to all the files and scripts deposited in the github. The creation layer corresponds to all the tools installed and used on the user's local machine (except for Ansible which is installed on the virtual machine but which could very well be installed on the local machine). The storage layer corresponds to the storage sites of the VMs (base and final). The instantiation layer corresponds to the instantiated virtual machine.","title":"Overview"},{"location":"basebox/","text":"Building the base VM (Base box) for Vagrant \u00b6 It is assumed that you have installed the Packer and Vagrant tools , as well as the Oracle VirtualBox virtualisation software. The operations for creating a base box are documented online; you have to follow the basic guide and apply the provider-specific rules, namely for VirtualBox . The procedure is somewhat tedious and, above all, an error can happen quickly. This is why we will use the Packer tool, which allows to fully automate the creation of a Vagrant base box by following the process illustrated by the figure below: Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . We are using Ubuntu version 18.04 as a basis. A Packer configuration is defined with a JSON file (box-config.json) with several sections: \"builders\": is used to define the elements for creating the virtual machine from an ISO image. \"provisioners\": is used to define the software configuration from Shell scripts to provision the virtual machine. \"post-processors\": runs once the virtual machine is created and provisioned. This allows among other things to define the output format of the VM. \"builders\" section To build this section, it is often simpler to start from already functional examples and to modify the few elements specific to its configuration. A query in a search engine (e.g. Google) with the keywords \"github packer ubuntu 18.04 virtualbox\" will give you enough examples. So we started with an example appropriate to our needs , which we then adapted. The VirtualBox box is made from an Ubuntu ISO specified by its URL (iso_urls) in order to create the VM in VirtualBox (\"type\": \"virtualbox-iso\") . A \"preseed\" file (http/preseed.cfg) allows to configure the installation (see more info on preseeding ). We have adapted the example by adding instructions concerning the root account and the network configuration. We limited the maximum disk size to 18GB (\"disk_size\": 18432) so that the final VM can be accepted by the Genouest cloud . \"provisioners\" section This section allows to run Shell scripts after the virtual machine has booted properly and then its operating system installed. So it is in this step that we can configure the VM to be compatible with Vagrant and VirtualBox. A Shell script (scripts/setup.sh) is then executed in order to: install the drivers for VirtualBox configure SSH accesses in compliance with Vagrant boxes. \"post-processors\" section Once the base virtual machine is fully installed and configured, it is simply exported to Vagrant format. Creating the base VM \u00b6 Once the configuration has been established, simply run Packer as follows: $> packer build box-config.json All the messages produced can be consulted on the github repository . The base VM after execution can be found under the builds directory. Registering a box on Vagrant Cloud \u00b6 Vagrant Cloud provides an API that allows users to register their virtual machines (boxes) with Vagrant Cloud so that they can be reused by themselves or by other users. The use of the API is described online . The registration of a virtual machine can only be done with the Vagrant format, i.e. a \"box\". A \"box\" is in fact a zipped archive file (TAR + GZIP) with the extension '.box'. Registering a \"box\" consists of 5 steps: Creating an entry for the \"box\": at least the name of the box (boxname) must be specified. Its description is optional. Creating a version: there may be several versions of the same entry; the version must be specified. Creation of a provider: similarly, for an entry (boxname) and a version, there may be several associated providers; the provider must be specified. Upload the box file. Validate the box. (entry + version + provider) All of these steps can be done either via the Vagrant Cloud web interface, or through multiple API calls. In order to facilitate the automation of registrations, the Vagrant tool provides a 'cloud' functionality that allows you to register your box on Vagrant Cloud. This requires an account (called an 'organisation') to be created on Vagrant Cloud. Example of invocation : $> vagrant cloud auth login ... Vagrant Cloud username: GAEV Vagrant Cloud password: XXXXXXX $> vagrant cloud publish GAEV/centos7-dd8Gb 1.0.0 virtualbox virtualbox-centos7_8Gb.box You are about to create a box on Vagrant Cloud with the following options: GAEV/centos7-dd8Gb (1.0.0) for virtualbox Automatic Release: true Do you wish to continue? [y/N] y Creating a box entry... Creating a version entry... Creating a provider entry... Uploading provider with file /Vagrant/boxes/virtualbox-centos7_8Gb.box Releasing box... Complete! tag: GAEV/centos7-dd8Gb username: GAEV name: centos7-dd8Gb private: false downloads: 0 created_at: 2020-07-25T17:53:04.340Z updated_at: 2020-07-25T18:01:10.665Z current_version: 1.0.0 providers: virtualbox The registered box can be viewed on Vagrant Cloud .","title":"Building the Base VM"},{"location":"basebox/#building-the-base-vm-base-box-for-vagrant","text":"It is assumed that you have installed the Packer and Vagrant tools , as well as the Oracle VirtualBox virtualisation software. The operations for creating a base box are documented online; you have to follow the basic guide and apply the provider-specific rules, namely for VirtualBox . The procedure is somewhat tedious and, above all, an error can happen quickly. This is why we will use the Packer tool, which allows to fully automate the creation of a Vagrant base box by following the process illustrated by the figure below: Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . We are using Ubuntu version 18.04 as a basis. A Packer configuration is defined with a JSON file (box-config.json) with several sections: \"builders\": is used to define the elements for creating the virtual machine from an ISO image. \"provisioners\": is used to define the software configuration from Shell scripts to provision the virtual machine. \"post-processors\": runs once the virtual machine is created and provisioned. This allows among other things to define the output format of the VM. \"builders\" section To build this section, it is often simpler to start from already functional examples and to modify the few elements specific to its configuration. A query in a search engine (e.g. Google) with the keywords \"github packer ubuntu 18.04 virtualbox\" will give you enough examples. So we started with an example appropriate to our needs , which we then adapted. The VirtualBox box is made from an Ubuntu ISO specified by its URL (iso_urls) in order to create the VM in VirtualBox (\"type\": \"virtualbox-iso\") . A \"preseed\" file (http/preseed.cfg) allows to configure the installation (see more info on preseeding ). We have adapted the example by adding instructions concerning the root account and the network configuration. We limited the maximum disk size to 18GB (\"disk_size\": 18432) so that the final VM can be accepted by the Genouest cloud . \"provisioners\" section This section allows to run Shell scripts after the virtual machine has booted properly and then its operating system installed. So it is in this step that we can configure the VM to be compatible with Vagrant and VirtualBox. A Shell script (scripts/setup.sh) is then executed in order to: install the drivers for VirtualBox configure SSH accesses in compliance with Vagrant boxes. \"post-processors\" section Once the base virtual machine is fully installed and configured, it is simply exported to Vagrant format.","title":"Building the base VM (Base box) for Vagrant"},{"location":"basebox/#creating-the-base-vm","text":"Once the configuration has been established, simply run Packer as follows: $> packer build box-config.json All the messages produced can be consulted on the github repository . The base VM after execution can be found under the builds directory.","title":"Creating the base VM"},{"location":"basebox/#registering-a-box-on-vagrant-cloud","text":"Vagrant Cloud provides an API that allows users to register their virtual machines (boxes) with Vagrant Cloud so that they can be reused by themselves or by other users. The use of the API is described online . The registration of a virtual machine can only be done with the Vagrant format, i.e. a \"box\". A \"box\" is in fact a zipped archive file (TAR + GZIP) with the extension '.box'. Registering a \"box\" consists of 5 steps: Creating an entry for the \"box\": at least the name of the box (boxname) must be specified. Its description is optional. Creating a version: there may be several versions of the same entry; the version must be specified. Creation of a provider: similarly, for an entry (boxname) and a version, there may be several associated providers; the provider must be specified. Upload the box file. Validate the box. (entry + version + provider) All of these steps can be done either via the Vagrant Cloud web interface, or through multiple API calls. In order to facilitate the automation of registrations, the Vagrant tool provides a 'cloud' functionality that allows you to register your box on Vagrant Cloud. This requires an account (called an 'organisation') to be created on Vagrant Cloud. Example of invocation : $> vagrant cloud auth login ... Vagrant Cloud username: GAEV Vagrant Cloud password: XXXXXXX $> vagrant cloud publish GAEV/centos7-dd8Gb 1.0.0 virtualbox virtualbox-centos7_8Gb.box You are about to create a box on Vagrant Cloud with the following options: GAEV/centos7-dd8Gb (1.0.0) for virtualbox Automatic Release: true Do you wish to continue? [y/N] y Creating a box entry... Creating a version entry... Creating a provider entry... Uploading provider with file /Vagrant/boxes/virtualbox-centos7_8Gb.box Releasing box... Complete! tag: GAEV/centos7-dd8Gb username: GAEV name: centos7-dd8Gb private: false downloads: 0 created_at: 2020-07-25T17:53:04.340Z updated_at: 2020-07-25T18:01:10.665Z current_version: 1.0.0 providers: virtualbox The registered box can be viewed on Vagrant Cloud .","title":"Registering a box on Vagrant Cloud"},{"location":"cloud/","text":"GenOuest is a national bioinformatics platform federated by the French Institute of Bioinformatics (IFB). This platform offers cloud services for the French public research community. Any researcher from this community can make a request to Genouest or any other IFB platform to have access to the proposed services. GenOuest offers on its datacenter infrastructure a service of providing computing resources in the form of virtual machines. The infrastructure is managed using Openstack , which is a set of open source software that allows the deployment of cloud computing infrastructures (Infrastructure as a Service, IaS). It is therefore necessary to have a valid account on this infrastructure (see the online help ). It is then assumed that you have installed Python (\u22652.7) as well as the package python-openstackclient . Retrieving its connection settings from GenOuest Openstack \u00b6 To set the required environment variables for openstack command line clients, you must download the environment file called openrc.sh from the GenOuest openStack dashboard as a user. This project-specific environment file contains the credentials that all openstack services use. Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . You need to get the openrc.sh script as well as the cloud.yml configuration file. The latter is to be put in the <home directory>/.config/openstack directory. Then run the openrc.sh script in the current shell (i.e invoking using a dot); you will be asked for your password: $> . ./openrc.sh Please enter your OpenStack Password for project <Project> as user <Users>: <Project> and <User> corresponding to your configuration. We will define some aliases to simplify the following commands: OS_SCRIPTS=<path of the python-openstackclient scripts> alias ostack=\"$OS_SCRIPTS/openstack --os-cloud=openstack --os-password $OS_PASSWORD\" alias onova=\"$OS_SCRIPTS/nova --os-password $OS_PASSWORD\" If everything is configured correctly, the following command should provide you with a list of available flavors. Flavours define the hardware configuration available for a server. It defines the size of a virtual server that can be started. $> ostack flavor list +------------+-------+------+-------+-----------+ | Name | RAM | Disk | VCPUs | Is Public | +------------+-------+------+-------+-----------+ | m2.xlarge | 16384 | 20 | 4 | True | | m1.small | 2048 | 20 | 1 | True | | m2.large | 8192 | 20 | 2 | True | | m1.medium | 4096 | 20 | 2 | True | | m2.4xlarge | 65536 | 20 | 8 | True | | m2.medium | 4096 | 20 | 1 | True | | m1.2xlarge | 32768 | 20 | 8 | True | | m1.large | 8192 | 20 | 4 | True | | m1.xlarge | 16384 | 20 | 8 | True | | m2.2xlarge | 32768 | 20 | 4 | True | +------------+-------+------+-------+-----------+ We will then proceed in two steps: create a VM image within the library create an instance of this image Creating the image on the openstack infrastructure \u00b6 It is first necessary to extract the VM file in VMDK format from the archive generated in the previous step. $> tar xvzf ubuntu-box.tar.gz box-disk001.vmdk This is the file that will be used to create the image. You must also specify a name for the image. IMAGE_NAME=jupyterhub-image ostack image create --disk-format vmdk --file box-disk001.vmdk $IMAGE_NAME ostack image set $IMAGE_NAME ostack image show $IMAGE_NAME Creating an instance from the created image \u00b6 From the created image, it is now possible to create an instance. You have to specify the name of the instance, its flavor (see above) and the SSH key associated to this VM. The SSH key (keypair) must first have been created from the openstack dashboard. You can list the SSH keys with the command: ostack keypair list You can also specify the file containing the commands to be executed after the first boot. Here this file is necessary ( user-data-jupystack.txt ) because we must overwrite the file /usr/local/bin/get-hostname giving the complete name of the instance on openstack in order to build the root URL of the jupyterhub application (cf jupyterhub.pre and jupyterhub.service ). In our example, we choose a flavor corresponding to 8 CPUs, 16GB of RAM and 20GB of disk. An SSH key has been created with the name genostack in the openstack dashboard. This SSH key must be a valid key on your (linux) openstack.genouest.org account(file ~/.ssh/authorized_keys ). IMAGE_NAME=jupyterhub-image SERVER_NAME=jupystack KEYPAIR=genostack FLAVOR_NAME=m1.xlarge IMAGEID=$(ostack image show $IMAGE_NAME | \\ grep \"| id \" | cut -d'|' -f3 | \\ sed -e \"s/ //g\") FLAVORID=$(ostack flavor list | \\ grep \"$FLAVOR_NAME\" | cut -d'|' -f2 | \\ sed -e \"s/ //g\") onova boot --flavor $FLAVORID --image $IMAGEID --security-groups default \\ --user-data ./user-data-jupystack.txt \\ --key-name $KEYPAIR $SERVER_NAME ostack server show $SERVER_NAME The last command is used to obtain the IP number of the VM thus created. Let's suppose that this IP number is 192.168.101.79 . You can then access the application from your web browser at the URL: https://app-192-168-101-79.vm.openstack.genouest.org/hub/login Acknowledgements \u00b6 We would like to thank the IFB GenOuest bioinformatics for providing storage and computing resources on its national life science Cloud.","title":"Using the Final VM on an Openstack cloud"},{"location":"cloud/#retrieving-its-connection-settings-from-genouest-openstack","text":"To set the required environment variables for openstack command line clients, you must download the environment file called openrc.sh from the GenOuest openStack dashboard as a user. This project-specific environment file contains the credentials that all openstack services use. Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . You need to get the openrc.sh script as well as the cloud.yml configuration file. The latter is to be put in the <home directory>/.config/openstack directory. Then run the openrc.sh script in the current shell (i.e invoking using a dot); you will be asked for your password: $> . ./openrc.sh Please enter your OpenStack Password for project <Project> as user <Users>: <Project> and <User> corresponding to your configuration. We will define some aliases to simplify the following commands: OS_SCRIPTS=<path of the python-openstackclient scripts> alias ostack=\"$OS_SCRIPTS/openstack --os-cloud=openstack --os-password $OS_PASSWORD\" alias onova=\"$OS_SCRIPTS/nova --os-password $OS_PASSWORD\" If everything is configured correctly, the following command should provide you with a list of available flavors. Flavours define the hardware configuration available for a server. It defines the size of a virtual server that can be started. $> ostack flavor list +------------+-------+------+-------+-----------+ | Name | RAM | Disk | VCPUs | Is Public | +------------+-------+------+-------+-----------+ | m2.xlarge | 16384 | 20 | 4 | True | | m1.small | 2048 | 20 | 1 | True | | m2.large | 8192 | 20 | 2 | True | | m1.medium | 4096 | 20 | 2 | True | | m2.4xlarge | 65536 | 20 | 8 | True | | m2.medium | 4096 | 20 | 1 | True | | m1.2xlarge | 32768 | 20 | 8 | True | | m1.large | 8192 | 20 | 4 | True | | m1.xlarge | 16384 | 20 | 8 | True | | m2.2xlarge | 32768 | 20 | 4 | True | +------------+-------+------+-------+-----------+ We will then proceed in two steps: create a VM image within the library create an instance of this image","title":"Retrieving its connection settings from GenOuest Openstack"},{"location":"cloud/#creating-the-image-on-the-openstack-infrastructure","text":"It is first necessary to extract the VM file in VMDK format from the archive generated in the previous step. $> tar xvzf ubuntu-box.tar.gz box-disk001.vmdk This is the file that will be used to create the image. You must also specify a name for the image. IMAGE_NAME=jupyterhub-image ostack image create --disk-format vmdk --file box-disk001.vmdk $IMAGE_NAME ostack image set $IMAGE_NAME ostack image show $IMAGE_NAME","title":"Creating the image on the openstack infrastructure"},{"location":"cloud/#creating-an-instance-from-the-created-image","text":"From the created image, it is now possible to create an instance. You have to specify the name of the instance, its flavor (see above) and the SSH key associated to this VM. The SSH key (keypair) must first have been created from the openstack dashboard. You can list the SSH keys with the command: ostack keypair list You can also specify the file containing the commands to be executed after the first boot. Here this file is necessary ( user-data-jupystack.txt ) because we must overwrite the file /usr/local/bin/get-hostname giving the complete name of the instance on openstack in order to build the root URL of the jupyterhub application (cf jupyterhub.pre and jupyterhub.service ). In our example, we choose a flavor corresponding to 8 CPUs, 16GB of RAM and 20GB of disk. An SSH key has been created with the name genostack in the openstack dashboard. This SSH key must be a valid key on your (linux) openstack.genouest.org account(file ~/.ssh/authorized_keys ). IMAGE_NAME=jupyterhub-image SERVER_NAME=jupystack KEYPAIR=genostack FLAVOR_NAME=m1.xlarge IMAGEID=$(ostack image show $IMAGE_NAME | \\ grep \"| id \" | cut -d'|' -f3 | \\ sed -e \"s/ //g\") FLAVORID=$(ostack flavor list | \\ grep \"$FLAVOR_NAME\" | cut -d'|' -f2 | \\ sed -e \"s/ //g\") onova boot --flavor $FLAVORID --image $IMAGEID --security-groups default \\ --user-data ./user-data-jupystack.txt \\ --key-name $KEYPAIR $SERVER_NAME ostack server show $SERVER_NAME The last command is used to obtain the IP number of the VM thus created. Let's suppose that this IP number is 192.168.101.79 . You can then access the application from your web browser at the URL: https://app-192-168-101-79.vm.openstack.genouest.org/hub/login","title":"Creating an instance from the created image"},{"location":"cloud/#acknowledgements","text":"We would like to thank the IFB GenOuest bioinformatics for providing storage and computing resources on its national life science Cloud.","title":"Acknowledgements"},{"location":"finalvm/","text":"Starting from an existing base VM (Base box) \u00b6 Creating a base VM can be complex for a non-expert. This is why it is easier to start from a base VM when it is available. This is also why the base VM should be as generic as possible so that it can be easily used for many applications. The base VM generated and used in this use-case was deposited on Vagrant Cloud , using its API (see Registering a box on Vagrant Cloud ). Building the final VM \u00b6 We will use the base VM to provision it, i.e. install and configure all the system and application tools (packages) necessary to make the JupyterHub application work properly. To do this, we will use the Ansible tool as illustrated in the figure below: Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . The first step is to create a Vagrantfile and customise it to suit your needs. This file follows the syntax of the Ruby language . A light version is given below: Vagrant.configure(\"2\") do |config| config.vm.box = file://builds/virtualbox-ubuntu1804.box config.vm.hostname = jupyterhub config.vm.network \"private_network\", type: \"dhcp\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\" config.vm.provision \"ansible_local\" do |ansible| ansible.playbook = \"ansible/playbook.yml\" ansible.install = true ansible.limit = 'all' end config.vm.provision \"shell\", path: \"scripts/cleanup.sh\" end config.vm.box sets the base box as input config.vm.hostname defines the hostname of the machine config.vm.network defines the network configuration in DHCP mode config.ssh.insert_key defines the SSH key. By default, it will be the one defined in the base box. config.vm.synced_folder defines a local directory as shared with the VM. At least, the \".\" directory must be defined in order to configure the VM. These shared directories will only be active during the creation and then test stage before export. config.vm.provision defines the process by which the VM will be provisioned. Here, two processes are invoked. i) 'ansible_local': indicates that the ansible tool will first be installed on the VM and then use the playbook defined in ansible/playbook; ii) 'shell': indicates that a shell script defined by its relative path will be used. Provisioning using the Ansible tool \u00b6 The choice was made not to install Ansible on the host machine used to create the VM, but rather it will be installed on the VM itself, this is to simplify the process. The provisioning is defined from a YAML file called playbook.yml --- - hosts: all become: true become_user: root #gather_facts: no vars_files: - vars/all.yml environment: PYTHONHTTPSVERIFY: 0 roles: - repositories - server - vm - install-r - jupyterhub - r_pkgs - python_pkgs Ansible allows tasks to be organised in a directory structure called a Role . In this configuration, playbooks invoke roles, which themselves define a set of tasks, so you can always group tasks together and reuse roles in other playbooks. Roles also allow you to collect templates, static files and variables along with your tasks in a structured format. Each role, and then each task within roles, is interpreted sequentially. Thus the following roles are defined: repositories configures the repositories for binary packages (systems, R, python, tools, ...), server configure the timezone vm configure the hostname install-r installs the R application with basic packages jupyterhub installs and configures the jupyterhub application r_pkgs install a set of R packages python_pkgs install a set of Python packages Each role is defined by a set of tasks, themselves defined by files in YAML format. For example, the role ' repositories ' is defined by the following roles/repositories/tasks/main.yml file: --- - name: Add an apt key by id from a keyserver apt_key: keyserver: \"{{repository.keyserver}}\" id: \"{{repository.id}}\" - name: Add repositories apt_repository: repo: \"{{item}}\" with_items: \"{{repository.repos}}\" Two tasks are thus defined and described by their name field. Then the actual task is defined using a name referring to a module . Hundreds of modules are available in Ansible. Here two modules are invoked: apt_key : add or remove an apt key apt_repository : add or remove a binary package repository. The variables defined by the double braces {{ }} correspond to those defined in the vars/all.yml file itself declared in the playbook. Thus the following lines can be found in this file: repository: keyserver: hkp://keyserver.ubuntu.com:80 id: E298A3A825C0D65DFD57CBB651716619E084DAB9 repos: - ppa:c2d4u.team/c2d4u4.0+ - ppa:hnakamur/libgit2 - deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ So this is how the provisioning task set is defined by Ansible. The ' jupyterhub ' role was built from the instructions provided on the TLJH website. The list of Python and R packages needed for a specific use under JupyterHub are to be defined in the vars/all.yml file , whose installation is taken care of by the ' python_pkgs ' and ' r_pkgs ' roles respectively. Shell script provisioning \u00b6 Once the provisioning by Ansible is finished, we proceed to a VM cleanup, i.e. uninstall Ansible, clean the temporary files and clean the hard disk. We use a Shell script ( scripts/cleanup.sh ) for this. Creating the final VM \u00b6 Once the configuration is established, you just have to execute the following commands: $> vagrant up $> vagrant package --output ubuntu-box.tar.gz All the messages produced can be consulted on the github repository","title":"Building the Final VM"},{"location":"finalvm/#starting-from-an-existing-base-vm-base-box","text":"Creating a base VM can be complex for a non-expert. This is why it is easier to start from a base VM when it is available. This is also why the base VM should be as generic as possible so that it can be easily used for many applications. The base VM generated and used in this use-case was deposited on Vagrant Cloud , using its API (see Registering a box on Vagrant Cloud ).","title":"Starting from an existing base VM (Base box)"},{"location":"finalvm/#building-the-final-vm","text":"We will use the base VM to provision it, i.e. install and configure all the system and application tools (packages) necessary to make the JupyterHub application work properly. To do this, we will use the Ansible tool as illustrated in the figure below: Note : All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository . The first step is to create a Vagrantfile and customise it to suit your needs. This file follows the syntax of the Ruby language . A light version is given below: Vagrant.configure(\"2\") do |config| config.vm.box = file://builds/virtualbox-ubuntu1804.box config.vm.hostname = jupyterhub config.vm.network \"private_network\", type: \"dhcp\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\" config.vm.provision \"ansible_local\" do |ansible| ansible.playbook = \"ansible/playbook.yml\" ansible.install = true ansible.limit = 'all' end config.vm.provision \"shell\", path: \"scripts/cleanup.sh\" end config.vm.box sets the base box as input config.vm.hostname defines the hostname of the machine config.vm.network defines the network configuration in DHCP mode config.ssh.insert_key defines the SSH key. By default, it will be the one defined in the base box. config.vm.synced_folder defines a local directory as shared with the VM. At least, the \".\" directory must be defined in order to configure the VM. These shared directories will only be active during the creation and then test stage before export. config.vm.provision defines the process by which the VM will be provisioned. Here, two processes are invoked. i) 'ansible_local': indicates that the ansible tool will first be installed on the VM and then use the playbook defined in ansible/playbook; ii) 'shell': indicates that a shell script defined by its relative path will be used.","title":"Building the final VM"},{"location":"finalvm/#provisioning-using-the-ansible-tool","text":"The choice was made not to install Ansible on the host machine used to create the VM, but rather it will be installed on the VM itself, this is to simplify the process. The provisioning is defined from a YAML file called playbook.yml --- - hosts: all become: true become_user: root #gather_facts: no vars_files: - vars/all.yml environment: PYTHONHTTPSVERIFY: 0 roles: - repositories - server - vm - install-r - jupyterhub - r_pkgs - python_pkgs Ansible allows tasks to be organised in a directory structure called a Role . In this configuration, playbooks invoke roles, which themselves define a set of tasks, so you can always group tasks together and reuse roles in other playbooks. Roles also allow you to collect templates, static files and variables along with your tasks in a structured format. Each role, and then each task within roles, is interpreted sequentially. Thus the following roles are defined: repositories configures the repositories for binary packages (systems, R, python, tools, ...), server configure the timezone vm configure the hostname install-r installs the R application with basic packages jupyterhub installs and configures the jupyterhub application r_pkgs install a set of R packages python_pkgs install a set of Python packages Each role is defined by a set of tasks, themselves defined by files in YAML format. For example, the role ' repositories ' is defined by the following roles/repositories/tasks/main.yml file: --- - name: Add an apt key by id from a keyserver apt_key: keyserver: \"{{repository.keyserver}}\" id: \"{{repository.id}}\" - name: Add repositories apt_repository: repo: \"{{item}}\" with_items: \"{{repository.repos}}\" Two tasks are thus defined and described by their name field. Then the actual task is defined using a name referring to a module . Hundreds of modules are available in Ansible. Here two modules are invoked: apt_key : add or remove an apt key apt_repository : add or remove a binary package repository. The variables defined by the double braces {{ }} correspond to those defined in the vars/all.yml file itself declared in the playbook. Thus the following lines can be found in this file: repository: keyserver: hkp://keyserver.ubuntu.com:80 id: E298A3A825C0D65DFD57CBB651716619E084DAB9 repos: - ppa:c2d4u.team/c2d4u4.0+ - ppa:hnakamur/libgit2 - deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ So this is how the provisioning task set is defined by Ansible. The ' jupyterhub ' role was built from the instructions provided on the TLJH website. The list of Python and R packages needed for a specific use under JupyterHub are to be defined in the vars/all.yml file , whose installation is taken care of by the ' python_pkgs ' and ' r_pkgs ' roles respectively.","title":"Provisioning using the Ansible tool"},{"location":"finalvm/#shell-script-provisioning","text":"Once the provisioning by Ansible is finished, we proceed to a VM cleanup, i.e. uninstall Ansible, clean the temporary files and clean the hard disk. We use a Shell script ( scripts/cleanup.sh ) for this.","title":"Shell script provisioning"},{"location":"finalvm/#creating-the-final-vm","text":"Once the configuration is established, you just have to execute the following commands: $> vagrant up $> vagrant package --output ubuntu-box.tar.gz All the messages produced can be consulted on the github repository","title":"Creating the final VM"},{"location":"google-cloud/","text":"Using Google Cloud SDK \u00b6 Before you begin \u00b6 In order to use the final VM on GCP, you first need to go through the following steps: Assuming your project is created (GCP), Assuming your SSH Keys defined at the level project (GCP) Assuming your SSH keys are available (added with ssh-add , a ssh-agent running), Assuming the Google Cloud SDK installed on your machine (See How to install Google Cloud SDK ) Quick overview \u00b6 We start from the VM final box and will then proceed in two steps: Upload the final box VM file (vmdk) to Cloud Storage and convert it to an image Create an instance of this image Create an image from the VM final box \u00b6 Preparation \u00b6 A number of variables need to be defined in order to set up the process correctly. The values indicated correspond to the test performed. Some of them must therefore be adapted to your context (OS, resources). Define the full path to the google-cloud utility to simplify further commands. GCLOUD = /cygdrive/c/_Tools/gcloud/google-cloud-sdk/bin/gcloud Initialization of variables concerning the project. You must adapt these values based on your own project PROJECT = test-gaev-20210906 ZONE = europe-west1-b Initialization of variables concerning the VM. The path (VDISK_DIR) and the name of the vmdk file (VMDK_FILE) must be changed according to your case. The type of virtual machines (VM_MACHINE) can also be adapted according to your needs (see machine families ) IMAGE = jupyterhub-img VM = jupyterhub-vm VDISK_DIR = /cygdrive/c/VirtualMach/Vagrant/jupyterhub/builds/vm VMDK_FILE = box-disk001.vmdk VM_OS = ubuntu-1804 VM_MACHINE = c2-standard-4 Set the project by default export CLOUDSDK_CORE_PROJECT = $PROJECT Get the project number PROJECT_NUMBER = $( GCLOUD iam service-accounts list | grep EMAIL | cut -d ':' -f2 | cut -d '-' -f1 | sed -e \"s/ //g\" | tr -d \"[\\n\\r]\" ) Set the Identity and Access Management ( IAM ) permissions. (See Access control overview and Service accounts for more details) $GCLOUD services enable cloudbuild.googleapis.com $GCLOUD projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $PROJECT_NUMBER @cloudbuild.gserviceaccount.com \\ --role roles/compute.admin $GCLOUD projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $PROJECT_NUMBER @cloudbuild.gserviceaccount.com \\ --role roles/iam.serviceAccountUser $GCLOUD projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $PROJECT_NUMBER @cloudbuild.gserviceaccount.com \\ --role roles/iam.serviceAccountTokenCreator Import the VMDK File \u00b6 Uses batch Windows command instead of cygwin bash. Indeed Python been an Anaconda tool for Windows, it does not correctly deal with files based on cygwin pathways, even if the pathway is simply the file name. Very strange! With this tip, it works. Perhaps we should have used the module for Anaconda instead (see google-cloud-sdk for Anaconda ). ( cd $VDISK_DIR echo \"@ECHO OFF CLS SET PATH=C:\\_Tools\\gcloud\\google-cloud-sdk\\bin;%PATH%; gcloud compute images import $IMAGE --zone $ZONE --no-guest-environment --source-file $VMDK_FILE --os $VM_OS exit /b 0 \" > /tmp/cmd.bat chmod +x /tmp/cmd.bat /tmp/cmd.bat ) Output generated Copying [box-disk001.vmdk] to [gs://test-gaev-20210906-daisy-bkt/tmpimage/5044ee90-6a16-43dd-9eff-789bf333c82c-box-disk001.vmdk]... ..............................................................................................done. WARNING: Importing image. This may take up to 2 hours. Created [https://cloudbuild.googleapis.com/v1/projects/test-gaev-20210906/builds/c1a34fc5-d8d7-4804-be22-58508bef1f82]. Logs are available at [https://console.cloud.google.com/cloud-build/builds/c1a34fc5-d8d7-4804-be22-58508bef1f82?project=188926165780]. starting build \"c1a34fc5-d8d7-4804-be22-58508bef1f82\" [import-image]: 2021-09-07T14:30:25Z Creating Google Compute Engine disk from gs://test-gaev-20210906-daisy-bkt/tmpimage/5044ee90-6a16-43dd-9eff-789bf333c82c-box-disk001.vmdk [import-image]: 2021-09-07T14:38:23Z Finished creating Google Compute Engine disk [import-image]: 2021-09-07T14:38:23Z Inspecting disk for OS and bootloader [import-image]: 2021-09-07T14:40:17Z Inspection result=os_release:{cli_formatted:\"ubuntu-1804\" distro:\"ubuntu\" major_version:\"18\" minor_version:\"04\" architecture:X64 distro_id:UBUNTU} elapsed_time_ms:114577 os_count:1 [import-image]: 2021-09-07T14:40:17Z Cloud Build ID: [import-image]: 2021-09-07T14:40:35Z Making disk bootable on Google Compute Engine Checking in the GCP console List the images linked to the project $GCLOUD compute images list | grep -B1 -A3 $PROJECT Output generated NAME: jupyterhub-img PROJECT: test-gaev-20210906 FAMILY: DEPRECATED: STATUS: READY Checking in the GCP console Create an instance based on an imported image \u00b6 Launch the VM instance creation. The type of virtual machines (VM_MACHINE) can be adapted according to your needs (see machine families ) $GCLOUD compute instances create $VM --image-project $PROJECT --image $IMAGE \\ --tags = http-server,https-server --zone = $ZONE --machine-type $VM_MACHINE Output generated Created [https://www.googleapis.com/compute/v1/projects/test-gaev-20210906/zones/europe-west1-b/instances/jupyterhub-vm]. NAME: jupyterhub-vm ZONE: europe-west1-b MACHINE_TYPE: c2-standard-4 PREEMPTIBLE: INTERNAL_IP: 10.132.0.25 EXTERNAL_IP: 35.187.120.148 STATUS: RUNNING Checking in the GCP console Get the IP address of the instance IP = $( $GCLOUD compute instances describe $VM \\ --format = 'get(networkInterfaces[0].accessConfigs[0].natIP)' | tr -d \"\\n\" | tr -d \"\\r\" ) Remove previous ssh keys for this IP in the known_hosts file grep -E -v \"^ $IP \" ~/.ssh/known_hosts > ~/.ssh/known_hosts.tmp cat ~/.ssh/known_hosts.tmp > ~/.ssh/known_hosts ssh -o 'StrictHostKeyChecking no' root@ $IP \"hostname\" Customization : Fixe the issue concerning the external IP address Because the IP address found by default is the internal one we need the external IP address. So we have to modify the get-hostname script for that. ssh root@ $IP \"echo \\\"echo $IP \\\" > /usr/local/bin/get-hostname\" Then reboot the VM ssh root@ $IP \"reboot\" Launch Jupyterhub in your web browser","title":"Using the Final VM on Google Cloud Platform"},{"location":"google-cloud/#using-google-cloud-sdk","text":"","title":"Using Google Cloud SDK"},{"location":"google-cloud/#before-you-begin","text":"In order to use the final VM on GCP, you first need to go through the following steps: Assuming your project is created (GCP), Assuming your SSH Keys defined at the level project (GCP) Assuming your SSH keys are available (added with ssh-add , a ssh-agent running), Assuming the Google Cloud SDK installed on your machine (See How to install Google Cloud SDK )","title":"Before you begin"},{"location":"google-cloud/#quick-overview","text":"We start from the VM final box and will then proceed in two steps: Upload the final box VM file (vmdk) to Cloud Storage and convert it to an image Create an instance of this image","title":"Quick overview"},{"location":"google-cloud/#create-an-image-from-the-vm-final-box","text":"","title":"Create an image from the VM final box"},{"location":"google-cloud/#preparation","text":"A number of variables need to be defined in order to set up the process correctly. The values indicated correspond to the test performed. Some of them must therefore be adapted to your context (OS, resources). Define the full path to the google-cloud utility to simplify further commands. GCLOUD = /cygdrive/c/_Tools/gcloud/google-cloud-sdk/bin/gcloud Initialization of variables concerning the project. You must adapt these values based on your own project PROJECT = test-gaev-20210906 ZONE = europe-west1-b Initialization of variables concerning the VM. The path (VDISK_DIR) and the name of the vmdk file (VMDK_FILE) must be changed according to your case. The type of virtual machines (VM_MACHINE) can also be adapted according to your needs (see machine families ) IMAGE = jupyterhub-img VM = jupyterhub-vm VDISK_DIR = /cygdrive/c/VirtualMach/Vagrant/jupyterhub/builds/vm VMDK_FILE = box-disk001.vmdk VM_OS = ubuntu-1804 VM_MACHINE = c2-standard-4 Set the project by default export CLOUDSDK_CORE_PROJECT = $PROJECT Get the project number PROJECT_NUMBER = $( GCLOUD iam service-accounts list | grep EMAIL | cut -d ':' -f2 | cut -d '-' -f1 | sed -e \"s/ //g\" | tr -d \"[\\n\\r]\" ) Set the Identity and Access Management ( IAM ) permissions. (See Access control overview and Service accounts for more details) $GCLOUD services enable cloudbuild.googleapis.com $GCLOUD projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $PROJECT_NUMBER @cloudbuild.gserviceaccount.com \\ --role roles/compute.admin $GCLOUD projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $PROJECT_NUMBER @cloudbuild.gserviceaccount.com \\ --role roles/iam.serviceAccountUser $GCLOUD projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $PROJECT_NUMBER @cloudbuild.gserviceaccount.com \\ --role roles/iam.serviceAccountTokenCreator","title":"Preparation"},{"location":"google-cloud/#import-the-vmdk-file","text":"Uses batch Windows command instead of cygwin bash. Indeed Python been an Anaconda tool for Windows, it does not correctly deal with files based on cygwin pathways, even if the pathway is simply the file name. Very strange! With this tip, it works. Perhaps we should have used the module for Anaconda instead (see google-cloud-sdk for Anaconda ). ( cd $VDISK_DIR echo \"@ECHO OFF CLS SET PATH=C:\\_Tools\\gcloud\\google-cloud-sdk\\bin;%PATH%; gcloud compute images import $IMAGE --zone $ZONE --no-guest-environment --source-file $VMDK_FILE --os $VM_OS exit /b 0 \" > /tmp/cmd.bat chmod +x /tmp/cmd.bat /tmp/cmd.bat ) Output generated Copying [box-disk001.vmdk] to [gs://test-gaev-20210906-daisy-bkt/tmpimage/5044ee90-6a16-43dd-9eff-789bf333c82c-box-disk001.vmdk]... ..............................................................................................done. WARNING: Importing image. This may take up to 2 hours. Created [https://cloudbuild.googleapis.com/v1/projects/test-gaev-20210906/builds/c1a34fc5-d8d7-4804-be22-58508bef1f82]. Logs are available at [https://console.cloud.google.com/cloud-build/builds/c1a34fc5-d8d7-4804-be22-58508bef1f82?project=188926165780]. starting build \"c1a34fc5-d8d7-4804-be22-58508bef1f82\" [import-image]: 2021-09-07T14:30:25Z Creating Google Compute Engine disk from gs://test-gaev-20210906-daisy-bkt/tmpimage/5044ee90-6a16-43dd-9eff-789bf333c82c-box-disk001.vmdk [import-image]: 2021-09-07T14:38:23Z Finished creating Google Compute Engine disk [import-image]: 2021-09-07T14:38:23Z Inspecting disk for OS and bootloader [import-image]: 2021-09-07T14:40:17Z Inspection result=os_release:{cli_formatted:\"ubuntu-1804\" distro:\"ubuntu\" major_version:\"18\" minor_version:\"04\" architecture:X64 distro_id:UBUNTU} elapsed_time_ms:114577 os_count:1 [import-image]: 2021-09-07T14:40:17Z Cloud Build ID: [import-image]: 2021-09-07T14:40:35Z Making disk bootable on Google Compute Engine Checking in the GCP console List the images linked to the project $GCLOUD compute images list | grep -B1 -A3 $PROJECT Output generated NAME: jupyterhub-img PROJECT: test-gaev-20210906 FAMILY: DEPRECATED: STATUS: READY Checking in the GCP console","title":"Import the VMDK File"},{"location":"google-cloud/#create-an-instance-based-on-an-imported-image","text":"Launch the VM instance creation. The type of virtual machines (VM_MACHINE) can be adapted according to your needs (see machine families ) $GCLOUD compute instances create $VM --image-project $PROJECT --image $IMAGE \\ --tags = http-server,https-server --zone = $ZONE --machine-type $VM_MACHINE Output generated Created [https://www.googleapis.com/compute/v1/projects/test-gaev-20210906/zones/europe-west1-b/instances/jupyterhub-vm]. NAME: jupyterhub-vm ZONE: europe-west1-b MACHINE_TYPE: c2-standard-4 PREEMPTIBLE: INTERNAL_IP: 10.132.0.25 EXTERNAL_IP: 35.187.120.148 STATUS: RUNNING Checking in the GCP console Get the IP address of the instance IP = $( $GCLOUD compute instances describe $VM \\ --format = 'get(networkInterfaces[0].accessConfigs[0].natIP)' | tr -d \"\\n\" | tr -d \"\\r\" ) Remove previous ssh keys for this IP in the known_hosts file grep -E -v \"^ $IP \" ~/.ssh/known_hosts > ~/.ssh/known_hosts.tmp cat ~/.ssh/known_hosts.tmp > ~/.ssh/known_hosts ssh -o 'StrictHostKeyChecking no' root@ $IP \"hostname\" Customization : Fixe the issue concerning the external IP address Because the IP address found by default is the internal one we need the external IP address. So we have to modify the get-hostname script for that. ssh root@ $IP \"echo \\\"echo $IP \\\" > /usr/local/bin/get-hostname\" Then reboot the VM ssh root@ $IP \"reboot\" Launch Jupyterhub in your web browser","title":"Create an instance based on an imported image"}]}